{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.0a0+82fd1c8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_label(a,b):\n",
    "    div = [ai/bi for ai,bi in zip(a,b)]\n",
    "    return div\n",
    "\n",
    "def rescale_label(a,b):\n",
    "    div = [ai*bi for ai,bi in zip(a,b)]\n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def resize_img_label(image, label = (0., 0.), target_size=(256,256)):\n",
    "    w_orig, h_orig = image.size\n",
    "    w_target, h_target = target_size\n",
    "    cx, cy = label\n",
    "    \n",
    "    # resize image and label\n",
    "    image_new = TF.resize(image, target_size)\n",
    "    label_new = cx/w_orig*w_target, cy/h_orig*h_target\n",
    "    \n",
    "    return image_new, label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(image, label, params):\n",
    "    image, label = resize_img_label(image,label,params[\"target_size\"])\n",
    "    \n",
    "    if params[\"scale_label\"]:\n",
    "        label=scale_label(label, params[\"target_size\"])\n",
    "        \n",
    "    image = TF.to_tensor(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMD_dataset(Dataset):\n",
    "    def __init__(self, path2data, transform, trans_params):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return size of dataset\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, path2data, transform, trans_params):\n",
    "    \n",
    "    # full path of the labels file\n",
    "    path2labels = os.path.join(path2data, \"Training400\", \"Fovea_location.xlsx\")\n",
    "    \n",
    "    # read labels as a data frame\n",
    "    labels_df = pd.read_excel(path2labels,index_col=\"ID\")\n",
    "    \n",
    "    # extract labels\n",
    "    self.labels = labels_df[[\"Fovea_X\", \"Fovea_Y\"]].values\n",
    "    \n",
    "    # extract ID and imgName columns\n",
    "    self.imgName = labels_df[\"imgName\"]\n",
    "    self.ids = labels_df.index\n",
    "    \n",
    "    self.fullPath2img = [0]*len(self.ids)\n",
    "    for id_ in self.ids:\n",
    "        if self.imgName[id_][0] == \"A\":\n",
    "            prefix = \"AMD\"\n",
    "        else:\n",
    "            prefix = \"Non-AMD\"\n",
    "        \n",
    "        self.fullPath2img[id_-1] = os.path.join(path2data, \"Training400\", prefix, self.imgName[id_])\n",
    "    \n",
    "    self.transform = transform\n",
    "    self.trans_params = trans_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    # load PIL image\n",
    "    image = Image.open(self.fullPath2img[idx])\n",
    "    label = self.labels[idx]\n",
    "    \n",
    "    # transform to tensor\n",
    "    image, label = self.transform(image, label, self.trans_params)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMD_dataset.__init__ = __init__\n",
    "AMD_dataset.__getitem__ = __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2data = \"./data/\"\n",
    "trans_params_val = {\n",
    "    \"target_size\": (256, 256),\n",
    "    \"p_hflip\": 0.0,\n",
    "    \"p_vflip\": 0.0,\n",
    "    \"p_shift\": 0.0,\n",
    "    \"p_brightness\": 0.0,\n",
    "    \"p_contrast\": 0.0,\n",
    "    \"p_gamma\": 0.0,\n",
    "    \"gamma\": 0.0,\n",
    "    \"scale_label\": True,\n",
    "}\n",
    "amd_ds2 = AMD_dataset(path2data, transformer, trans_params_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "----------\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "sss = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "indices = range(len(amd_ds2))\n",
    "for train_index, val_index in sss.split(indices):\n",
    "    print(len(train_index))\n",
    "    print(\"-\"*10)\n",
    "    print(len(val_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "val_ds = Subset(amd_ds2, val_index)\n",
    "print(len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "val_dl = DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Net, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, params):\n",
    "    super(Net, self).__init__()\n",
    "    \n",
    "    C_in, H_in, W_in = params[\"input_shape\"]\n",
    "    init_f = params[\"initial_filters\"]\n",
    "    num_outputs = params[\"num_outputs\"]\n",
    "    \n",
    "    self.conv1 = nn.Conv2d(C_in, init_f, kernel_size=3, stride=2, padding=1)\n",
    "    self.conv2 = nn.Conv2d(init_f + C_in, 2 * init_f, kernel_size=2, stride=1, padding=1)\n",
    "    self.conv3 = nn.Conv2d(3 * init_f + C_in, 4 * init_f, kernel_size=3, padding=1)\n",
    "    self.conv4 = nn.Conv2d(7 * init_f + C_in, 8 * init_f, kernel_size=3, padding=1)\n",
    "    self.conv5 = nn.Conv2d(15 * init_f + C_in, 16 * init_f, kernel_size=3, padding=1)\n",
    "    self.fc1 = nn.Linear(16 * init_f, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    identity = F.avg_pool2d(x, 4, 4)\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = F.max_pool2d(x, 2, 2)\n",
    "    x = torch.cat((x, identity), dim=1)\n",
    "    \n",
    "    identity = F.avg_pool2d(x, 2, 2)\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = F.max_pool2d(x, 2, 2)\n",
    "    x = torch.cat((x, identity), dim=1)\n",
    "    \n",
    "    identity = F.avg_pool2d(x, 2, 2)\n",
    "    x = F.relu(self.conv3(x))\n",
    "    x = F.maxpool2d(x, 2, 2)\n",
    "    x = torch.cat((x, identity), dim=1)\n",
    "    \n",
    "    identity = F.avg_pool2d(x, 2, 2)\n",
    "    x = F.relu(self.conv4(x))\n",
    "    x = F.max_pool2d(x, 2, 2)\n",
    "    x = torch.cat((x, identity), dim=1)\n",
    "    \n",
    "    x = F.relu(self.conv5(x))\n",
    "    \n",
    "    x = F.adaptive_avg_pool2d(x, 1)\n",
    "    x = x.reshape(x.size(0), -1)\n",
    "    \n",
    "    x = self.fc1(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net.__init__ = __init__\n",
    "Net.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(19, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(51, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(115, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(243, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_model = {\n",
    "    \"input_shape\": (3, 256, 256),\n",
    "    \"initial_filters\": 16,\n",
    "    \"num_outputs\": 2,\n",
    "}\n",
    "\n",
    "# create model\n",
    "model = Net(params_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the model to cuda/gpu device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
